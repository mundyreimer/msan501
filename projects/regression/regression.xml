<chapter title="Predicting Death Rates With Gradient Descent"
         author={[Terence Parr](http://parrt.cs.usfca.edu)}>


<section title="Goal">

The goal of this project is to extend the techniques we learned in class, for one-dimensional gradient descent, to a two-dimensional domain space, training a machine learning model called *linear regression*.  This problem is also known as *curve fitting*.  As part of this project, you will learn how to compute with vectors instead of scalars. Please use starter kit file [regression-starterkit.ipynb](https://github.com/parrt/msan501/blob/master/projects/regression/regression-starterkit.ipynb). You will be doing your work in your repository `regression-userid` cloned from github.
			 
<section title="Discussion">
	
<subsection title="Problem statement">

Given training data $(\vec x_i, y_i)$ for $i=1..N$ samples with dependent variable $y_i$, we would like to predict a $y$ for some $\vec x$ not in our training set. $\vec x$ is generally a vector of independent variables, but we'll use a scalar in this exercise. If we assume there is a linear relationship between $\vec x$ and $y$, then we can draw a line through the data and predict future values with that line function. To do that, we need to compute the two parameters of our model: a slope and a $y$ intercept. (We will see the model below.)

For example, let's compare the number of people who died by becoming tangled in their bedsheets versus the per capita cheese consumption. (Data is from [spurious correlations](http://www.tylervigen.com/spurious-correlations).) Here is the raw data:

<pyeval label="ex" hide=true>
import numpy as np
from mpl_toolkits.mplot3d import Axes3D # required even though not ref'd!
</pyeval>

<pyeval label="ex" output="df">
import pandas as pd
df = pd.read_csv('data/cheese_deaths.csv')
</pyeval>

If we plot the data across years, we see an obvious linear relationship:

<pyfig label="ex" hide=true width="70%">
from matplotlib import rcParams
rcParams["font.size"] = 12

fig, ax1 = plt.subplots(figsize=(8,2.5))
ax1.set_xticks(df.years)
p1, = ax1.plot(df.years, df.cheese, 'darkred', marker='d', markersize=5)
ax2 = ax1.twinx()
p2, = ax2.plot(df.years, df.deaths, 'k', marker='o', markersize=5)

ax1.set_xlabel("Year", fontsize=12)
ax1.set_ylabel("Cheese (lbs)", fontsize=12)
ax2.set_ylabel("Bedsheet deaths", fontsize=12)

ax1.legend(handles=[p1, p2], labels=['cheese consumption','bedsheet deaths'],
           fontsize=12)
</pyfig>

We can also do a scatterplot of cheese versus deaths:

<pyfig label="ex" hide=true width="60%">
rcParams["font.size"] = 12
fig, ax = plt.subplots(figsize=(8,2.5))
ax.plot(df.cheese, df.deaths)
ax.set_xlabel('cheese consumption (lbs)')
ax.set_ylabel('deaths')
</pyfig>

<subsection title="Best fit line that minimizes squared error">

Recall the formula for a line from high school: $y = m x + b$.  We normally rewrite that using elements of a vector coefficients, $\vec b$ (in bold), in preparation for describing it with vector notation from linear algebra. For simplicity,  though, we'll stick with scalar coefficients for now:

\[
\hat{y} = b_2 x + b_1
\]

We use notation $\hat{y}$ to indicate that it approximates $y$ from our data set but is not necessarily equal to any of the true target $y_i$.

The "best line" is one that minimizes some cost function that compares the known each $y_i$ value at $x_i$ to the predicted $\hat{y}$ of the linear model that we conjure up using parameters $\vec b = [b_1, b_2]$ where $b_1$ is the y-intercept and $b_2$ is the slope of the line. A good measure is the *mean squared error*. The cost function adds up all of these squared errors across $N$ observations to tell us how good of a fit our linear model is:

\[
Cost(\vec b) = \frac{1}{N}\sum_{i=1}^{N}(\underbrace{\hat{y}}_\text{linear model} - \overbrace{y_i}^\text{true value})^2
\]

Inlining the expression for our model, we get:

\[
Cost(\vec b) = \frac{1}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)^2
\]

As we wiggle the linear model parameters, $b_1$ and $b_2$, the value of the cost function will change.  The following graphs shows the errors/residuals that are squared and averaged to get the overall cost for two different "curve fits."

<pyeval label="ex" hide=true>
def line_fit(B, x):
    return B[2-1]*x + B[1-1]

fit = np.polyfit(df.cheese, df.deaths, deg=1)
bestline = np.poly1d(fit)
bestline_coeff = np.array([bestline.coefficients[1], bestline.coefficients[0]])
</pyeval>

<pyeval label="ex" hide=true>
def Cost(B,X,Y):
    "Line coefficients: B = [y-intercept, slope]"
    cost = 0.0
    for x,y in zip(X,Y):
        y_ = line_fit(B,x)
        cost += (y_ - y)**2
    return cost/len(X)
</pyeval>

<pyeval label="ex" hide=true>
def plot_residuals(B):
	rcParams["font.size"] = 12
	fig, ax = plt.subplots(figsize=(5,2.5))
	#y = bestline(df.cheese.values)
	y = line_fit(B,df.cheese.values)
	ax.scatter(df.cheese, df.deaths, linewidth=.5, s=15)
	for c,d in zip(df.cheese, df.deaths):
	    plt.plot([c,c],[line_fit(B,c),d], color='red', linewidth=.5)
	gline, = plt.plot(df.cheese,y,'--',color='grey', linewidth=1)
	ax.set_xlabel('cheese consumption')
	ax.set_ylabel('deaths')
	ax.legend(loc='upper left',
			handles=[gline],
			labels=[f"Equation is $y = {B[1]:.1f}x + {B[0]:.1f}$"],
			fontsize=12)
    ax.set_title(f"Residuals plot; Cost = {Cost(B,df.cheese, df.deaths):.1f}")
</pyeval>

<table>
<tr>
<td>
<pyfig label="ex" hide=true width="100%">
plot_residuals(bestline_coeff)
</pyfig>
<td>
<pyfig label="ex" hide=true width="100%">
plot_residuals([600, 0])
</pyfig>
</table>

The good news is that we know the cost function is a quadratic (by construction), which is convex and has an exact solution. All we have to do is figure out where the cost function flattens out. Mathematically, that is when the partial derivatives of the cost function are both zero. The partial derivative for a particular dimension is just the rate of change of that dimension at a particular spot. It's like a skier examining the slope of his or her skis against the mountain one at a time. (For more, see [The intuition behind gradient descent](http://explained.ai/gradient-boosting/descent.html#sec:3.1) in an article that explains gradient boosting, which you will probably look at for your second machine learning course.)

\[
\nabla Cost(\vec b) = \vec 0
\]

Instead of solving that equation for $\vec b$ symbolically, we'll use gradient descent to minimize the cost function. It's important to learn about this numerical technique because there are lots and lots of cost functions that are not simple little quadratics with symbolic solutions.  For example, if we change this problem from a predictor to a classifier (*logistic regression*), then training requires an iterative method like gradient descent.

To show our prediction model in action, we can ask how many deaths there would be in  if we consumed an average of 32 lbs of cheese.  To make a prediction, all we have to do is plug $x=32$ into the equation for a best fit line, $\hat{y} = 113.1 x - 2977.3$, which gives us 641.9 deaths.

<subsection title="Gradient descent in 3D">

Before trying to minimize the cost function, it's helpful to study what the surface looks like in three dimensions, as shown in the following graphs. The X and Y dimensions are the coefficients, $\vec b$, of our linear model and the Z coordinate (up) is the cost function.

<pyeval label="ex" hide=true>
b1 = np.arange(-6000, 4000, 50)  # y intercept
b2 = np.arange(-200, 300, 1)   # slope
(b1_mesh, b2_mesh) = np.meshgrid(b1, b2, indexing='ij')
C = np.zeros(b1_mesh.shape)
#b0_mesh.shape, b1_mesh.shape, C.shape           

for i in range(len(b1)):
        for j in range(len(b2)):
                c = Cost([b1[i],b2[j]],X=df.cheese.values, Y=df.deaths.values)
                C[i][j] = c / 1e8 # scale down the cost
</pyeval>

<pyeval label="ex" hide=true>
def plot3d(elev=50, azim=145, show_best_text=True):
    rcParams["font.size"] = 12
    fig = plt.figure(figsize=(8,7))
    ax = fig.add_subplot(111, projection='3d')
    ax.view_init(elev, azim)
    surface = ax.plot_surface(b1_mesh, b2_mesh, C, alpha=0.6, cmap='coolwarm')
    #surface = ax.contour(b1_mesh, b2_mesh, C, alpha=0.5, cmap='coolwarm')
    
    plt.title("""$Cost({\\bf b}) = \sum_{i=1}^{N}({b_2 x_i + b_1} - {y_i})^2$""")
    ax.set_xlabel('$b_1$ y-intercept', fontsize=16)
    ax.set_ylabel('$b_2$ slope', fontsize=16)
    ax.set_zlabel('$Cost({\\bf b})$ in $10^8$ units', rotation='vertical', fontsize=18)

    # show projections
    #cset = ax.contour(b0_mesh, b1_mesh, C, zdir='z', offset=-0.2, cmap='coolwarm')
    # cset = ax.contour(b0_mesh, b1_mesh, C, zdir='x', offset=-6000, cmap='coolwarm')
    # cset = ax.contour(b0_mesh, b1_mesh, C, zdir='y', offset=300, cmap='coolwarm')

    b = bestline_coeff
    ax.plot([b[0]], [b[1]], Cost(b, X=df.cheese.values, Y=df.deaths.values) / 1e8, "ro")
    if show_best_text:
        ax.text(b[0]+1000, b[1], Cost(b, X=df.cheese.values, Y=df.deaths.values) / 1e8,
                "Optimal ${\\bf b} = ["+str(int(b[0]))+", "+str(int(b[1]))+"]$",
               fontsize=16)
    plt.tight_layout()
</pyeval>

<table>
<tr>
<td>
<pyfig label="ex" hide=true width="100%">
plot3d()
</pyfig>
<td>
<pyfig label="ex" hide=true width="100%">
plot3d(elev=30, azim=0, show_best_text=False)
</pyfig>
</table>

<pyfig label="ex" hide=true width="50%">
plot3d(elev=0, azim=145, show_best_text=False)
</pyfig>

What surprised me is that changes to the slope of the linear model's slope coefficient $b_2$, away from the optimal $b_2=113.1$, cost much more than tweaks to the y intercept, $b_1$. Regardless, the surface is convex. If you look at the third graph, you can see that at a 45Â° angle, the surface is a very shallow bowl, not a valley or saddle. Unfortunately, based upon the deep trough that grows slowly along the diagonal of $(b_1,b_2)$, gradient descent takes a while to converge to the minimum. We will examine the path of gradient descent for a few initial starting points.

The recurrence relation for updating our estimate of $\vec b=[b_1, b_2]$ that minimizes $Cost(\vec b)$ is the same as the single-variable gradient-descent we did in class but with vectors for $\eta$ and $\vec b$ instead of scalars:

\[
\vec b_{t+1} = \vec b_t - \eta \nabla Cost(\vec b_t)
\]

where

\[
\nabla Cost(\vec b) = \begin{bmatrix}
\frac{\partial}{\partial b_1} Cost(\left[ b_1 \above 0pt{b_2} \right])\vspace{3mm}\\
\frac{\partial}{\partial b_2} Cost(\left[ b_1 \above 0pt{b_2} \right])\\
\end{bmatrix}
\]

The symbolic partial derivatives with respect to the individual model parameters are: 

<latex>
\begin{eqnarray*}
\frac{\partial}{\partial b_1} Cost(\left[ b_1 \above 0pt{b_2} \right]) &=& \frac{\partial}{\partial b_1}\frac{1}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)^2\\
&=& \frac{1}{N}\sum_{i=1}^{N}\frac{\partial}{\partial b_1}(b_2 x_i + b_1 - y_i)^2\\
&=& \frac{1}{N}\sum_{i=1}^{N}2(b_2 x_i + b_1 - y_i)\frac{\partial}{\partial b_1}(b_2 x_i + b_1 - y_i)\\
&=& \frac{1}{N}\sum_{i=1}^{N}2(b_2 x_i + b_1 - y_i)\\
&=& \frac{2}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)\\
\end{eqnarray*}
</latex>

and

<latex>
\begin{eqnarray*}
\frac{\partial}{\partial b_2} Cost(\left[ b_1 \above 0pt{b_2} \right]) &=& \frac{\partial}{\partial b_2}\frac{1}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)^2\\
&=& \frac{1}{N}\sum_{i=1}^{N}\frac{\partial}{\partial b_2}(b_2 x_i + b_1 - y_i)^2\\
&=& \frac{1}{N}\sum_{i=1}^{N}2(b_2 x_i + b_1 - y_i)\frac{\partial}{\partial b_2}(b_2 x_i + b_1 - y_i)\\
&=& \frac{1}{N}\sum_{i=1}^{N}2(b_2 x_i + b_1 - y_i)x_i\\
&=& \frac{2}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)x_i\\
\end{eqnarray*}
</latex>

Combining those two partial derivatives into a vector gives us the gradient:

\[
\nabla Cost(\vec b) =
\begin{bmatrix}
\frac{2}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i) \vspace{3mm}\\
\frac{2}{N}\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)x_i\\
\end{bmatrix}
\]

Note that we could approximate that symbolic gradient using partial finite differences (to avoid calculus) if we wanted, but it's more expensive:

\[
\nabla Cost(\vec b) =
\begin{bmatrix}
\frac{\partial}{b_1}{Cost(\left[ b_1 \above 0pt{b_2} \right])}\\
\frac{\partial}{b_2}{Cost(\left[ b_1 \above 0pt{b_2} \right])} \\
\end{bmatrix}
\approx
\begin{bmatrix}
\frac{Cost(\left[ b_1+\Delta b \above 0pt{b_2} \right]) - Cost(\left[ b_1 \above 0pt{b_2} \right])}{\Delta b} \vspace{3mm}\\
\frac{Cost(\left[ b_1 \above 0pt{b_2+\Delta b} \right]) - Cost(\left[ b_1 \above 0pt{b_2} \right])}{\Delta b} \\
\end{bmatrix}
\]

The $\Delta b$ variable is a very small delta step away from a specific coordinate in each dimension in turn.

<subsection title="Gradient-descent algorithm">

The minimization algorithm for the mean squared error cost function, $Cost$, taking vector $\vec b$ looks like:

<latex>
\setlength{\algomargin}{3pt}
\SetAlCapSkip{-10pt}
\begin{algorithm}[H]
\LinesNumbered
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\SetInd{.5em}{.5em}
\TitleOfAlgo{{\em minimize}($\vec b_0 = [b_1, b_2]$, $\eta$, {\em precision}) {\bf returns} coefficents $\vec b$}
Let $\vec b = \vec b_0$\\
\Repeat{$||(Cost(\vec b)-Cost(\vec b_{prev}))|| < precision$}{
Let $\nabla Cost = \begin{bmatrix}
\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i) \vspace{3mm}\\
\sum_{i=1}^{N}(b_2 x_i + b_1 - y_i)x_i\\
\end{bmatrix}\vspace{3mm}\\
Let $\vec b_{prev} = \vec b$\\
Let $\vec b = \vec b_{prev} - \eta \otimes \nabla Cost$\vspace{1mm}\\
}
\Return{$\vec b_{t+1}$}\\
\end{algorithm}
</latex>

The $\otimes$ operator is element-wise multiplication and is sometimes called the *Hadamard product*. (There isn't a standard notation for element-wise multiplication, unfortunately.) $\eta$ is a vector with learning rates for both directions. This is important because the curvature in the two dimensions is radically different. Using a learning rate that is small enough to prevent divergence in $b_2$ would be glacially slow for $b_1$.

To use this function, we just pick an initial $\vec b_0$ (at random if we want) and call minimize with a learning rate vector $\eta$ and the desired precision (max change in $f$ between iterations).

<table>
<tr>
	<td><img src="images/heatmap1.png" width="100%">
	<td><img src="images/heatmap2.png" width="100%">
</table>

The starting point is the red X and the ending point is the black X. Notice that the minimization algorithm overshoots in $b_2$ then comes back only to drift back towards its original overshoot. This is adjustable by tweaking the learning rate. For example, if we decimate the $b_2$ learning rate by 10, the graph appears to make a sharp turn rather than overshooting and coming back. However, minimization is dramatically slower and takes an order of magnitude more steps to arrive at the solution.

<section title="Your task">

You will use gradient descent to solve the linear regression problem above, using the same data from `data/cheese_deaths.csv`. As part of your final submission, you must provide heat maps with traces that indicate the steps taken by your gradient descent as I have shown above. In addition, you must display the learning rate vector beneath the statement about the number of steps like the following except with the question marks  replaced with your learning rates. 

<img src="images/heatmap3.png" width="45%">
	
Start with something very small for the learning rates and ramp up the values until the minimization algorithm diverges instead of converging, then back them down one notch. Move them individually to find the optimal values that converge but as fast as possible. Have your program choose random starting $b_0$ vectors.

**Deliverables**:  Download and fill in in the [regression-starterkit.ipynb](https://github.com/parrt/msan501/blob/master/projects/regression/regression-starterkit.ipynb) notebook, renaming it to `regression.ipynb`. Make sure that you have added it to your repository, `regression-userid`, committed it, and pushed to github. Here is the [starter kit with sample output](https://github.com/parrt/msan501/raw/master/projects/regression/regression-starterkit.pdf) (PDF). Make sure when you submit your notebook that it has been executed and shows all of the images and output.

**You must tweak the learning rate and precision so that your results agree at least with the first three decimal points of the analytic solution: 2004.4563215 to get credit for that component of the project.**  The cheese versus deaths plots are worth 20% and the minimization and related functions plus heat map with proper minimization trace is worth 80%.
	